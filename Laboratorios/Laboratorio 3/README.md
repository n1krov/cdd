¬°Perfecto! Empecemos con una introducci√≥n clara y luego profundizamos en cada paso.

## üî¨ Introducci√≥n al Laboratorio: Ingenier√≠a de Caracter√≠sticas de Texto

El laboratorio se centra en el **Procesamiento del Lenguaje Natural (NLP)** aplicado al _Machine Learning_.

### üéØ El Objetivo Central

El prop√≥sito del laboratorio es mostrar c√≥mo transformar la **informaci√≥n textual no estructurada** (como descripciones o comentarios) en **caracter√≠sticas predictivas num√©ricas**.


> En este contexto, una **caracter√≠stica** (o _feature_) es una **variable num√©rica y medible** que se extrae del texto crudo para que pueda ser utilizada como _input_ en un modelo de _machine learning_.

- **Texto a N√∫meros:** Dado que los modelos de _machine learning_ solo operan con datos num√©ricos, el texto debe ser cuantificado para poder ser utilizado como _input_ en estos modelos.

### üõ†Ô∏è Las T√©cnicas

El enfoque del cap√≠tulo es utilizar m√©todos de ingenier√≠a de caracter√≠sticas que permiten extraer informaci√≥n r√°pidamente de **textos cortos**, capturando su complejidad a trav√©s de par√°metros estad√≠sticos.


##### 1. Caracter√≠sticas Estructurales/Estil√≠sticas üìè

Estas caracter√≠sticas se centran en la **estructura f√≠sica y el estilo de escritura** del texto, ignorando el significado espec√≠fico de las palabras. Miden **c√≥mo est√° escrito** el texto.

- **Pregunta Clave:** ¬øC√≥mo est√° organizada la informaci√≥n?
    
- **Qu√© Miden:** **Par√°metros estad√≠sticos** del texto.
    
    - **Longitud:** ¬øEs un texto largo o corto? (N√∫mero de caracteres, n√∫mero de palabras).
        
    - **Diversidad:** ¬øQu√© tan variado es el vocabulario? (N√∫mero de palabras √∫nicas).
        
    - **Estructura:** ¬øCu√°ntas ideas completas contiene? (Conteo de oraciones).
        
- **Analog√≠a:** Es como clasificar un libro por su n√∫mero de p√°ginas, el tama√±o de la letra o la cantidad de cap√≠tulos, sin leer la trama.
    
- **Recetas del Laboratorio:** Cubiertas en la **Receta 1** (Conteo de palabras/caracteres) y **Receta 2** (Conteo de oraciones).
    

##### 2. Caracter√≠sticas de Contenido/Sint√°cticas üß†

Estas caracter√≠sticas se centran en el **contenido sem√°ntico** del texto y la **importancia de las palabras**. Miden **de qu√© trata** el texto.

- **Pregunta Clave:** ¬øQu√© palabras se usan y qu√© tan importantes son?
    
- **Qu√© Miden:** La presencia, la frecuencia y el peso de las palabras.
    
    - **Contenido:** Se registra qu√© palabras aparecen y cu√°ntas veces (Bag-of-Words - BoW).
        
    - **Importancia:** Se pondera la relevancia de cada palabra en relaci√≥n con el conjunto completo de documentos (TF-IDF).
        
- **Analog√≠a:** Es como clasificar un libro por las 100 palabras clave m√°s importantes que utiliza ("magia," "drag√≥n," "profec√≠a").
    
- **Recetas del Laboratorio:** Cubiertas en la **Receta 3** (Bag-of-Words), **Receta 4** (TF-IDF), y la **Receta 5** (Limpieza y Stemming, que es un paso de preparaci√≥n crucial para el contenido).



## üë©‚Äçüè´ Profundizaci√≥n en Cada Receta

Ahora, vamos a ver el detalle de cada una de las 5 "Recetas" del laboratorio.

### 1. Receta 1: Conteo de Caracteres, Palabras y Vocabulario

Esta es la forma m√°s b√°sica de cuantificar un texto y se realiza usando las funciones vectorizadas de _strings_ de **pandas**.

- **Implementaci√≥n en Python:**
    
    - **N√∫mero de Caracteres:** `df["text"].str.strip().str.len()`
        
    - **N√∫mero de Palabras:** `df["text"].str.split().str.len()`

	- 1. N√∫mero de Vocabulario (Palabras √önicas)
		Esta es una **medida absoluta** de la riqueza del vocabulario
		- **Definici√≥n:** Es el **conteo directo** de cu√°ntas palabras diferentes aparecen en el texto.
		- **Implementaci√≥n:** Se usa `len(set(palabras))`. El conjunto (`set`) elimina duplicados, dejando solo las palabras √∫nicas.
		- **Ejemplo:** En la frase: "La casa azul, la casa es grande", el vocabulario √∫nico es {'la', 'casa', 'azul', 'es', 'grande'}, dando un **N√∫mero de Vocabulario = 5**.
    

	-  2. Diversidad L√©xica

		Esta es una **medida relativa** que indica la tasa de **repetici√≥n** o **variedad** de palabras.

		- Definici√≥n: Es un cociente entre el n√∫mero total de palabras y el n√∫mero de palabras √∫nicas.
	    $$\text{Diversidad L√©xica} = \frac{\text{N√∫mero total de palabras}}{\text{N√∫mero de palabras √∫nicas}}$$
		- **Interpretaci√≥n:**
		    - Un valor **cercano a 1** indica **mucha diversidad**, es decir, se repiten muy pocas palabras (casi todas las palabras usadas son √∫nicas).
		    - Un valor **alto** (mayor que 1, a menudo 2, 3 o m√°s) indica **poca diversidad**; el texto es repetitivo y reutiliza las mismas palabras con frecuencia.
        
- **Ejemplo (Continuaci√≥n):**
    
    - N√∫mero total de palabras (contando repeticiones): 7 ("La", "casa", "azul", "la", "casa", "es", "grande")
    - N√∫mero de Vocabulario (√önicas): 5
    - **Diversidad L√©xica:** $7 / 5 = 1.4$. Un valor moderado que indica cierta repetici√≥n de palabras como 'la' y 'casa'.        

¬°Claro! La Receta 2 es muy sencilla y se enfoca en una idea: **contar cu√°ntas ideas completas tiene un texto** para medir su complejidad o densidad de informaci√≥n.

---

#### üìú Receta 2: Contar las Ideas (Conteo de Oraciones)

##### üéØ El Objetivo

El objetivo de esta receta es crear una **caracter√≠stica num√©rica** que represente la **cantidad de oraciones** que hay en un documento.

- **¬øPor qu√© importa?** Un texto con diez oraciones probablemente contiene m√°s informaci√≥n o es m√°s complejo que un texto con solo una oraci√≥n.
    

##### üîë El Proceso: Tokenizaci√≥n de Oraciones

Para contar las oraciones, se usa una t√©cnica de NLP llamada **Tokenizaci√≥n de Oraciones**.

1. **Herramienta:** Se utiliza la funci√≥n `sent_tokenize` de la librer√≠a **NLTK** (Natural Language Toolkit).
    
2. **Mecanismo:** Esta funci√≥n "rompe" o divide el texto cada vez que encuentra un signo de puntuaci√≥n que marca el final de una oraci√≥n (como un punto `.`, un signo de interrogaci√≥n `?` o un signo de exclamaci√≥n `!`).
    
3. **Resultado:** `sent_tokenize` devuelve una **lista** donde cada elemento es una oraci√≥n completa.
    

##### üíª Implementaci√≥n F√°cil (El C√≥digo)

El c√≥digo simplemente hace esto:

1. Aplica `sent_tokenize` al texto para obtener la lista de oraciones.
    
2. Cuenta **cu√°ntos elementos tiene esa lista** (`len()`).
    
3. Ese conteo es el valor num√©rico de la nueva caracter√≠stica (`num_sent`) para ese documento.
    

|**Texto Original**|**Oraciones Tokenizadas (Lista)**|**Caracter√≠stica Num√©rica**|
|---|---|---|
|"Hoy es martes. ¬øVendr√°s al laboratorio?"|`['Hoy es martes.', '¬øVendr√°s al laboratorio?']`|**2**|

##### üõë La Regla de Oro

La **Nota Importante** es crucial: **Este conteo debe hacerse primero.**

- **La raz√≥n:** Si haces la limpieza de texto (Receta 5) antes, eliminar√≠as los puntos, signos de interrogaci√≥n y exclamaci√≥n. Si no hay puntuaci√≥n, la funci√≥n `sent_tokenize` no tendr√° c√≥mo saber d√≥nde termina una idea y comienza la siguiente, y el conteo ser√° incorrecto.
    

**En resumen:** La Receta 2 nos da el n√∫mero de "ideas" que tiene el texto, y debe ser el primer paso de preprocesamiento que involucre la puntuaci√≥n.


---

### 3. Receta 3: Bag-of-Words (BoW) y N-grams

Este m√©todo convierte el texto en una gran matriz de conteos. Se utiliza el `CountVectorizer` de **scikit-learn**.

- **Pipeline de Transformaci√≥n:**
    
    1. **Limpieza Previa:** Se recomienda eliminar puntuaci√≥n y n√∫meros del texto antes de la vectorizaci√≥n.
        
    2. **Vectorizaci√≥n:**
        - `CountVectorizer` crea el vocabulario (todas las palabras √∫nicas) a partir de los documentos.
            
        - Cada palabra del vocabulario se convierte en una **columna** de la matriz resultante.
            
        - Cada fila representa un documento, y el valor es la **frecuencia absoluta** de la palabra en ese documento.
            
    3. **Configuraci√≥n Com√∫n:**
        
        - `stop_words="english"`: Ignora palabras comunes como 'the', 'a', 'is'.
            
        - `ngram_range=(1, 1)`: Indica que solo se considerar√°n palabras individuales (unigramas). Se podr√≠a usar `(1, 2)` para incluir palabras individuales y pares de palabras (bigramas).
            
        - `min_df` o `max_features`: Se utilizan para controlar el tama√±o de la matriz, eliminando palabras muy raras o muy comunes, o limitando el total de columnas.
            

---

### 4. Receta 4: Implementaci√≥n de TF-IDF

Esta t√©cnica refina el BoW asignando un peso a cada palabra seg√∫n su relevancia, utilizando el `TfidfVectorizer` de **scikit-learn**.

- **F√≥rmula Conceptual:** $\text{TF-IDF} = \text{TF} \times \text{IDF}$.
    
- **Term Frequency (TF):** La frecuencia de la palabra en el documento.
    
- **Inverse Document Frequency (IDF):** Una medida de cu√°n rara es la palabra en **toda la colecci√≥n** de documentos.
    

|**Componente**|**Describe**|**Impacto en el Peso**|
|---|---|---|
|**TF (Frecuencia)**|¬øCu√°nto aparece la palabra en _este_ documento?|Alto si aparece mucho.|
|**IDF (Rareza)**|¬øEn cu√°ntos otros documentos de la colecci√≥n aparece?|Alto si es rara (solo aparece en pocos documentos).|

- **Resultado:** El TF-IDF da un peso **bajo** a palabras muy comunes y un peso **alto** a palabras espec√≠ficas que son clave para ese documento en particular.
    

---

### 5. Receta 5: Limpieza y Stemming de Variables de Texto

Este es el proceso completo de preprocesamiento, crucial para estandarizar el texto antes de la vectorizaci√≥n (Recetas 3 y 4).

- **Pasos de Limpieza (en orden):**
    
    1. **Puntuaci√≥n y N√∫meros:** Se eliminan o reemplazan por espacios.
        
    2. **Min√∫sculas:** Conversi√≥n a _lowercase_.
        
    3. **Normalizaci√≥n de Espacios:** M√∫ltiples espacios se reducen a uno solo.
        
    4. **Eliminaci√≥n de Stop Words:** Se usa el conjunto de _stop words_ de NLTK para remover palabras como 'a', 'is', 'the'.
        
    5. **Stemming:** Reducci√≥n de las palabras a su ra√≠z usando un _Stemmer_ (como `SnowballStemmer`). Por ejemplo, 'computando', 'computadora' $\rightarrow$ 'comput'.
        
- **Importancia:** Un texto limpio (estandarizado) garantiza que, por ejemplo, 'Corriendo.' y 'correr' sean tratados como la misma caracter√≠stica ('corr') en el BoW o TF-IDF, mejorando la calidad del modelo.
    
