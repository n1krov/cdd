¬°Perfecto! Empecemos con una introducci√≥n clara y luego profundizamos en cada paso.

## üî¨ Introducci√≥n al Laboratorio: Ingenier√≠a de Caracter√≠sticas de Texto

El laboratorio se centra en el **Procesamiento del Lenguaje Natural (NLP)** aplicado al _Machine Learning_.

### üéØ El Objetivo Central

El prop√≥sito del laboratorio es mostrar c√≥mo transformar la **informaci√≥n textual no estructurada** (como descripciones o comentarios) en **caracter√≠sticas predictivas num√©ricas**.

- **Texto a N√∫meros:** Dado que los modelos de _machine learning_ solo operan con datos num√©ricos, el texto debe ser cuantificado para poder ser utilizado como _input_ en estos modelos.
    

### üõ†Ô∏è Las T√©cnicas

El enfoque del cap√≠tulo es utilizar m√©todos de ingenier√≠a de caracter√≠sticas que permiten extraer informaci√≥n r√°pidamente de **textos cortos**, capturando su complejidad a trav√©s de par√°metros estad√≠sticos.

Las t√©cnicas se dividen en dos grandes grupos:

1. **Caracter√≠sticas Estructurales/Estil√≠sticas:** Crean caracter√≠sticas a partir de par√°metros estad√≠sticos del texto, como la longitud de las palabras, el n√∫mero de palabras √∫nicas o el conteo de oraciones. (Cubierto en Recetas 1 y 2).
    
2. **Caracter√≠sticas de Contenido/Sint√°cticas:** Representan el contenido del texto, enfoc√°ndose en qu√© palabras se usan y qu√© tan importantes son, a trav√©s de m√©todos de vectorizaci√≥n como Bag-of-Words y TF-IDF. (Cubierto en Recetas 3, 4 y 5).
    

---

## üë©‚Äçüè´ Profundizaci√≥n en Cada Receta

Ahora, vamos a ver el detalle de cada una de las 5 "Recetas" del laboratorio.

### 1. Receta 1: Conteo de Caracteres, Palabras y Vocabulario

Esta es la forma m√°s b√°sica de cuantificar un texto y se realiza usando las funciones vectorizadas de _strings_ de **pandas**.

- **Implementaci√≥n en Python:**
    
    - **N√∫mero de Caracteres:** `df["text"].str.strip().str.len()`
        
    - **N√∫mero de Palabras:** `df["text"].str.split().str.len()`
        
    - **N√∫mero de Vocabulario (√önicas):** Se aplica `.str.lower()` (para unificar may√∫sculas/min√∫sculas) y luego se calcula la longitud del conjunto (`set`) de las palabras divididas.
        
    - **Diversidad L√©xica:** Se calcula como `N√∫mero total de palabras / N√∫mero de palabras √∫nicas`.
        
- **Utilidad:** Las descripciones m√°s largas y ricas en vocabulario √∫nico suelen contener m√°s informaci√≥n y pueden ser un predictor fuerte en algunos problemas.
    

---

### 2. Receta 2: Estimaci√≥n de la Complejidad por Conteo de Oraciones

Aqu√≠ se utiliza la librer√≠a **NLTK** para realizar la **tokenizaci√≥n de oraciones**.

- **Concepto Clave:** La `sent_tokenize` de NLTK divide el texto en oraciones individuales bas√°ndose en la puntuaci√≥n (puntos, signos de exclamaci√≥n/interrogaci√≥n).
    
- **Implementaci√≥n en Python:** Se define una funci√≥n que usa `nltk.tokenize.sent_tokenize(text)` y devuelve el n√∫mero de elementos en la lista resultante. Esta funci√≥n se aplica a la columna de texto del DataFrame usando `.apply()`.
    
- **Nota Importante:** Este paso **debe hacerse antes** de cualquier eliminaci√≥n de puntuaci√≥n o cambio de caso.
    

---

### 3. Receta 3: Bag-of-Words (BoW) y N-grams

Este m√©todo convierte el texto en una gran matriz de conteos. Se utiliza el `CountVectorizer` de **scikit-learn**.

- **Pipeline de Transformaci√≥n:**
    
    1. **Limpieza Previa:** Se recomienda eliminar puntuaci√≥n y n√∫meros del texto antes de la vectorizaci√≥n.
        
    2. **Vectorizaci√≥n:**
        
        - `CountVectorizer` crea el vocabulario (todas las palabras √∫nicas) a partir de los documentos.
            
        - Cada palabra del vocabulario se convierte en una **columna** de la matriz resultante.
            
        - Cada fila representa un documento, y el valor es la **frecuencia absoluta** de la palabra en ese documento.
            
    3. **Configuraci√≥n Com√∫n:**
        
        - `stop_words="english"`: Ignora palabras comunes como 'the', 'a', 'is'.
            
        - `ngram_range=(1, 1)`: Indica que solo se considerar√°n palabras individuales (unigramas). Se podr√≠a usar `(1, 2)` para incluir palabras individuales y pares de palabras (bigramas).
            
        - `min_df` o `max_features`: Se utilizan para controlar el tama√±o de la matriz, eliminando palabras muy raras o muy comunes, o limitando el total de columnas.
            

---

### 4. Receta 4: Implementaci√≥n de TF-IDF

Esta t√©cnica refina el BoW asignando un peso a cada palabra seg√∫n su relevancia, utilizando el `TfidfVectorizer` de **scikit-learn**.

- **F√≥rmula Conceptual:** $\text{TF-IDF} = \text{TF} \times \text{IDF}$.
    
- **Term Frequency (TF):** La frecuencia de la palabra en el documento.
    
- **Inverse Document Frequency (IDF):** Una medida de cu√°n rara es la palabra en **toda la colecci√≥n** de documentos.
    

|**Componente**|**Describe**|**Impacto en el Peso**|
|---|---|---|
|**TF (Frecuencia)**|¬øCu√°nto aparece la palabra en _este_ documento?|Alto si aparece mucho.|
|**IDF (Rareza)**|¬øEn cu√°ntos otros documentos de la colecci√≥n aparece?|Alto si es rara (solo aparece en pocos documentos).|

- **Resultado:** El TF-IDF da un peso **bajo** a palabras muy comunes y un peso **alto** a palabras espec√≠ficas que son clave para ese documento en particular.
    

---

### 5. Receta 5: Limpieza y Stemming de Variables de Texto

Este es el proceso completo de preprocesamiento, crucial para estandarizar el texto antes de la vectorizaci√≥n (Recetas 3 y 4).

- **Pasos de Limpieza (en orden):**
    
    1. **Puntuaci√≥n y N√∫meros:** Se eliminan o reemplazan por espacios.
        
    2. **Min√∫sculas:** Conversi√≥n a _lowercase_.
        
    3. **Normalizaci√≥n de Espacios:** M√∫ltiples espacios se reducen a uno solo.
        
    4. **Eliminaci√≥n de Stop Words:** Se usa el conjunto de _stop words_ de NLTK para remover palabras como 'a', 'is', 'the'.
        
    5. **Stemming:** Reducci√≥n de las palabras a su ra√≠z usando un _Stemmer_ (como `SnowballStemmer`). Por ejemplo, 'computando', 'computadora' $\rightarrow$ 'comput'.
        
- **Importancia:** Un texto limpio (estandarizado) garantiza que, por ejemplo, 'Corriendo.' y 'correr' sean tratados como la misma caracter√≠stica ('corr') en el BoW o TF-IDF, mejorando la calidad del modelo.
    
