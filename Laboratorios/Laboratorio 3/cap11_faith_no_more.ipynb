{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdfd07d-3914-4aa2-bac9-78d72c222d1b",
   "metadata": {},
   "source": [
    "## Resumen del Capítulo: Ingeniería de Características a partir de Texto\n",
    "\n",
    "### Introducción al Capítulo\n",
    "\n",
    "En muchos conjuntos de datos, la información puede provenir de campos de texto libre,\n",
    "como descripciones de incidentes o reseñas de clientes. A diferencia de los datos tabulares,\n",
    "el texto varía en longitud, contenido y estilo de escritura. El objetivo de este capítulo\n",
    "es mostrar cómo transformar esta información textual en características predictivas numéricas\n",
    "que pueden ser utilizadas en modelos de machine learning.\n",
    "\n",
    "Las técnicas cubiertas pertenecen al campo del **Procesamiento del Lenguaje Natural (NLP)**,\n",
    "que se ocupa de programar computadoras para comprender el lenguaje humano. En concreto,\n",
    "el capítulo se enfoca en métodos para extraer rápidamente características de textos cortos,\n",
    "capturando su complejidad a través de parámetros estadísticos (como la longitud de las palabras,\n",
    "el número de palabras únicas y el conteo de oraciones).\n",
    "\n",
    "### Librerías y Requisitos Técnicos\n",
    "\n",
    "| Librería           | Propósito                                                                                     |\n",
    "|--------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **pandas**         | Manipulación de datos y funciones vectorizadas de procesamiento de strings (str).            |\n",
    "| **scikit-learn**   | Carga de conjuntos de datos (e.g., 20 Newsgroup) y transformers clave.                      |\n",
    "| **NLTK**           | Herramienta integral de Python para NLP, esencial para la tokenización y stemming.          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12cf937a-f154-4b68-b5d5-dabe5f584065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/juani/Documentos/Facultad/Ciencia de\n",
      "[nltk_data]     Datos/proyectos/lab3/Faith No More/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/juani/Documentos/Facultad/Ciencia de\n",
      "[nltk_data]     Datos/proyectos/lab3/Faith No More/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS - Librerías y Módulos Necesarios\n",
    "# ============================================================================\n",
    "\n",
    "# --- Sistema Operativo y Rutas ---\n",
    "import os  # Operaciones del sistema operativo (no usado directamente, pero disponible)\n",
    "from pathlib import (\n",
    "    Path,\n",
    ")  # Manejo moderno y multiplataforma de rutas de archivos/directorios\n",
    "\n",
    "# --- Manipulación de Datos ---\n",
    "import pandas as pd  # Análisis y manipulación de datos en DataFrames\n",
    "# Usado para: crear tablas, aplicar operaciones vectorizadas en texto\n",
    "\n",
    "# --- Natural Language Toolkit (NLTK) - Core ---\n",
    "import nltk  # Librería principal de NLP para Python\n",
    "# Usado para: descargar recursos y configurar rutas de datos\n",
    "\n",
    "# --- NLTK - Tokenización ---\n",
    "from nltk.tokenize import sent_tokenize  # Divide texto en oraciones individuales\n",
    "# Usado en: Receta 2 para contar número de oraciones\n",
    "\n",
    "# --- NLTK - Stop Words ---\n",
    "from nltk.corpus import (\n",
    "    stopwords,\n",
    ")  # Acceso a listas de palabras comunes sin valor semántico\n",
    "# Usado en: Receta 5 para filtrar palabras como 'the', 'a', 'is'\n",
    "\n",
    "# --- NLTK - Stemming ---\n",
    "from nltk.stem.snowball import SnowballStemmer  # Reduce palabras a su raíz/base\n",
    "# Usado en: Receta 5 para convertir 'running', 'runs' -> 'run'\n",
    "\n",
    "# --- Scikit-learn - Datasets ---\n",
    "from sklearn.datasets import (\n",
    "    fetch_20newsgroups,\n",
    ")  # Descarga dataset de grupos de noticias\n",
    "# Usado para: obtener textos de ejemplo para análisis\n",
    "\n",
    "# --- Scikit-learn - Vectorización de Texto ---\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    ")  # Convierte texto a matriz Bag-of-Words\n",
    "# Usado en: Receta 3 para contar frecuencia de palabras\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer,\n",
    ")  # Convierte texto a matriz TF-IDF\n",
    "# Usado en: Receta 4 para ponderar importancia de palabras\n",
    "\n",
    "# Configuración de directorios del proyecto\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "NLTK_DATA_DIR = PROJECT_ROOT / \"nltk_data\"\n",
    "SKLEARN_DATA_DIR = PROJECT_ROOT / \"sklearn_data\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "NLTK_DATA_DIR.mkdir(exist_ok=True)\n",
    "SKLEARN_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configurar NLTK para usar el directorio del proyecto\n",
    "nltk.data.path.insert(0, str(NLTK_DATA_DIR))\n",
    "\n",
    "# Descargar recursos de NLTK en el directorio del proyecto\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "nltk.download(\"punkt\", download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "nltk.download(\"stopwords\", download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "nltk.download(\"punkt_tab\", download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "print(\"✓ Recursos de NLTK descargados\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5a141-fd06-4bf7-945e-597d4d44fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para cargar datos en el directorio del proyecto\n",
    "def load_newsgroups_data(subset=\"train\"):\n",
    "    \"\"\"\n",
    "    Carga el dataset 20 Newsgroups en el directorio del proyecto.\n",
    "\n",
    "    Parameters:\n",
    "        subset (str): 'train' o 'test'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columna 'text'\n",
    "    \"\"\"\n",
    "    print(f\"Cargando dataset 20 Newsgroups ({subset})...\")\n",
    "    data = fetch_20newsgroups(\n",
    "        subset=subset,\n",
    "        data_home=str(SKLEARN_DATA_DIR),\n",
    "        remove=(\"headers\", \"footers\", \"quotes\"),  # Limpieza inicial\n",
    "    )\n",
    "    df = pd.DataFrame(data.data, columns=[\"text\"])\n",
    "    print(f\"✓ Dataset cargado: {len(df)} documentos\\n\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f770d6-7ced-4e55-820f-126a393ab391",
   "metadata": {},
   "source": [
    "## Recetas Clave del Capítulo\n",
    "\n",
    "El capítulo se estructura alrededor de cinco recetas principales, que transforman \n",
    "el texto sin procesar en datos estructurados y numéricos.\n",
    "\n",
    "## **Receta 1:** Conteo de Caracteres, Palabras y Vocabulario\n",
    "\n",
    "Esta receta se centra en medir la complejidad del texto a través de estadísticas básicas. \n",
    "Las descripciones más largas y ricas en vocabulario único suelen contener más información.\n",
    "\n",
    "Características extraídas (usando pandas):\n",
    "1. **Número total de caracteres:** Incluye letras, números, símbolos y espacios.\n",
    "2. **Número total de palabras**.\n",
    "3. **Número total de palabras únicas (vocabulario)**.\n",
    "4. **Diversidad léxica:** Cociente entre el número total de palabras y el número de palabras únicas.\n",
    "5. **Longitud promedio de la palabra:** Cociente entre el número de caracteres y el número de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb7187-fd22-42d1-b775-2d98ec2f4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RECETA 1: Conteo de Caracteres, Palabras y Vocabulario\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cargar datos\n",
    "df = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Conteo de caracteres (después de strip para eliminar espacios en blanco)\n",
    "df[\"num_char\"] = df[\"text\"].str.strip().str.len()\n",
    "\n",
    "# Conteo de palabras (split() divide el texto en espacios en blanco)\n",
    "df[\"num_words\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "# Conteo de vocabulario (palabras únicas)\n",
    "# Usar lower() para evitar que 'Palabra' y 'palabra' sean tratadas como diferentes\n",
    "df[\"num_vocab\"] = df[\"text\"].str.lower().str.split().apply(lambda x: len(set(x)))\n",
    "\n",
    "# Diversidad Léxica (evitar división por cero)\n",
    "df[\"lexical_div\"] = df[\"num_words\"] / df[\"num_vocab\"].replace(0, 1)\n",
    "\n",
    "# Longitud Promedio de Palabras (evitar división por cero)\n",
    "df[\"ave_word_length\"] = df[\"num_char\"] / df[\"num_words\"].replace(0, 1)\n",
    "\n",
    "print(\"\\nEstadísticas básicas del texto:\")\n",
    "print(\n",
    "    df[\n",
    "        [\"num_char\", \"num_words\", \"num_vocab\", \"lexical_div\", \"ave_word_length\"]\n",
    "    ].describe()\n",
    ")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "print(df[[\"text\", \"num_char\", \"num_words\", \"num_vocab\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31cda7b-092d-48da-9a36-b346a03ba239",
   "metadata": {},
   "source": [
    "## **Receta 2:** Estimación de la Complejidad por Conteo de Oraciones\n",
    "\n",
    "Capturar el número de oraciones ofrece información sobre la cantidad de contenido en el texto, \n",
    "ya que las descripciones con múltiples oraciones tienden a ser más informativas. \n",
    "Este proceso se denomina **tokenización de oraciones**.\n",
    "\n",
    "**Nota importante:** La tokenización de oraciones se basa en la puntuación y la capitalización. \n",
    "Si planea contar oraciones, este paso debe realizarse antes de cualquier eliminación de \n",
    "puntuación o cambio de caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76d193-d434-499d-9dcb-38af6f0baece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 2: Conteo de Oraciones\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recargar datos limpios (sin headers/footers/quotes)\n",
    "df_sentences = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Tomar una muestra para demostración (más rápido)\n",
    "df_sentences = df_sentences.head(100).copy()\n",
    "\n",
    "\n",
    "# Función robusta para contar oraciones\n",
    "def count_sentences(text):\n",
    "    \"\"\"Cuenta oraciones manejando textos vacíos.\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return 0\n",
    "    try:\n",
    "        return len(sent_tokenize(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error al tokenizar: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Crear característica de número de oraciones\n",
    "df_sentences[\"num_sent\"] = df_sentences[\"text\"].apply(count_sentences)\n",
    "\n",
    "print(\"\\nEstadísticas de número de oraciones:\")\n",
    "print(df_sentences[\"num_sent\"].describe())\n",
    "print(\"\\nEjemplos:\")\n",
    "print(df_sentences[[\"text\", \"num_sent\"]].head(3).to_string(max_colwidth=60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2038c-85da-46b5-a269-4967952a2005",
   "metadata": {},
   "source": [
    "## **Receta 3:** Creación de Características con Bag-of-Words y N-grams\n",
    "\n",
    "El **Bag-of-Words (BoW)** es una representación simplificada donde cada palabra única \n",
    "se convierte en una variable, y su valor representa la frecuencia con la que aparece en el texto. \n",
    "El BoW captura la multiplicidad de palabras, pero no su orden o gramática.\n",
    "\n",
    "Para capturar algo de sintaxis, se usan N-grams, que son secuencias contiguas de n ítems \n",
    "(por ejemplo, 2-grams: \"Dogs like\", \"like cats\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7add5c3-93b2-4fae-b5ad-9c64ba36ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 3: Bag-of-Words\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recargar datos\n",
    "df_bow = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Limpieza preliminar: Eliminar puntuación y números\n",
    "# Reemplazar con espacio para evitar unir palabras\n",
    "df_bow[\"text_clean\"] = (\n",
    "    df_bow[\"text\"]\n",
    "    .str.replace(r\"[^\\w\\s]\", \" \", regex=True)  # Puntuación -> espacio\n",
    "    .str.replace(r\"\\d+\", \" \", regex=True)  # Números -> espacio\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)  # Múltiples espacios -> uno solo\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Configuración de CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 1),  # Solo unigramas (palabras simples)\n",
    "    min_df=0.05,  # Frecuencia mínima del 5%\n",
    "    max_features=100,  # Limitar a 100 features para demostración\n",
    ")\n",
    "\n",
    "# Ajuste y transformación\n",
    "X = vectorizer.fit_transform(df_bow[\"text_clean\"])\n",
    "\n",
    "# Captura del BoW en un DataFrame\n",
    "bagofwords = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\n",
    "    f\"\\nDimensiones de la matriz BoW: {bagofwords.shape[0]} filas x {bagofwords.shape[1]} columnas\"\n",
    ")\n",
    "print(\n",
    "    f\"Número de features (palabras únicas): {len(vectorizer.get_feature_names_out())}\"\n",
    ")\n",
    "print(\"\\nPrimeras 5 palabras más frecuentes:\")\n",
    "print(bagofwords.sum().sort_values(ascending=False).head())\n",
    "print(\"\\nPrimeras 3 filas del BoW:\")\n",
    "print(bagofwords.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d714f2-6a0b-4618-9e9c-a130215892f4",
   "metadata": {},
   "source": [
    "## **Receta 4:** Implementación de TF-IDF\n",
    "\n",
    "TF-IDF es una estadística numérica que mide la relevancia de una palabra en un documento \n",
    "específico dentro de una colección completa de documentos.\n",
    "\n",
    "• **Term Frequency (TF):** Simplemente la cuenta de la palabra en un texto individual.\n",
    "• **Inverse Document Frequency (IDF):** Mide cuán común es la palabra en todos los documentos. \n",
    "  Las palabras que aparecen en casi todos los documentos (como 'the' o 'a') tendrán un bajo peso.\n",
    "\n",
    "TF-IDF pondera la importancia; una palabra es importante si aparece mucho en un texto (tf alto) \n",
    "y pocas veces en el resto de los textos (idf alto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834a27b-9d97-4232-866c-491b2a8c71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 4: TF-IDF\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Usar los datos ya limpios de la receta anterior\n",
    "# Configuración de TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=0.05,\n",
    "    max_features=100,\n",
    ")\n",
    "\n",
    "# Ajuste y transformación\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_bow[\"text_clean\"])\n",
    "\n",
    "# Captura del TF-IDF en un DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nDimensiones de la matriz TF-IDF: {tfidf_df.shape[0]} filas x {tfidf_df.shape[1]} columnas\"\n",
    ")\n",
    "print(\"\\nPrimeras 3 filas del TF-IDF (valores normalizados):\")\n",
    "print(tfidf_df.head(3))\n",
    "\n",
    "# Comparar con BoW\n",
    "print(\"\\nComparación BoW vs TF-IDF para la primera fila:\")\n",
    "comparison = pd.DataFrame(\n",
    "    {\"BoW\": bagofwords.iloc[0].head(10), \"TF-IDF\": tfidf_df.iloc[0].head(10)}\n",
    ")\n",
    "print(comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb2f7c-6d42-425f-89f8-426732943708",
   "metadata": {},
   "source": [
    "## **Receta 5:** Limpieza y Stemming de Variables de Texto\n",
    "\n",
    "La limpieza o preprocesamiento del texto es crucial antes de crear características \n",
    "(como BoW o TF-IDF) para estandarizar el contenido y mejorar la precisión del modelo.\n",
    "\n",
    "**Pasos de Preprocesamiento:**\n",
    "1. **Eliminación de Puntuación y Números:** Se eliminan caracteres que no son letras o espacios.\n",
    "2. **Configuración de Caso (Lowercase):** Se establece todo el texto en minúsculas.\n",
    "3. **Eliminación de Stop Words:** Se remueven palabras comunes y funcionales.\n",
    "4. **Stemming:** Se reduce cada palabra a su raíz o base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc4ac1-d321-45d6-b9d3-a39c7324551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 5: Limpieza y Stemming Completo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recargar datos frescos\n",
    "df_clean = load_newsgroups_data(subset=\"train\")\n",
    "df_clean = df_clean.head(100).copy()  # Muestra para demostración\n",
    "\n",
    "# Paso 1: Eliminación de puntuación (reemplazar con espacio)\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
    "\n",
    "# Paso 2: Eliminación de números\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\d+\", \" \", regex=True)\n",
    "\n",
    "# Paso 3: Conversión a minúsculas\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.lower()\n",
    "\n",
    "# Paso 4: Normalizar espacios múltiples\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Paso 5: Función para eliminar Stop Words\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Elimina stop words de manera eficiente.\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    words = [word for word in text.split() if word not in STOP_WORDS]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "df_clean[\"text\"] = df_clean[\"text\"].apply(remove_stopwords)\n",
    "\n",
    "# Paso 6: Stemming (Reducción a la raíz)\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\"Aplica stemming a cada palabra.\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    words = [STEMMER.stem(word) for word in text.split()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "df_clean[\"text\"] = df_clean[\"text\"].apply(stem_words)\n",
    "\n",
    "print(\"\\nTexto limpio y stemmed completado.\")\n",
    "print(\"\\nEjemplo de transformación:\")\n",
    "print(\"\\nOriginal:\", df.iloc[0][\"text\"][:200])\n",
    "print(\"\\nLimpiado y stemmed:\", df_clean.iloc[0][\"text\"][:200])\n",
    "\n",
    "# Verificar que el texto está listo para feature extraction\n",
    "print(f\"\\n✓ {len(df_clean)} documentos procesados y listos para feature extraction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ddaa9-5412-4def-8b35-6f59bbf69c10",
   "metadata": {},
   "source": [
    "### Resumen del Pipeline Completo\n",
    "\n",
    "El proceso de feature engineering en texto es similar a un chef que prepara ingredientes \n",
    "para un plato complejo. Inicialmente, tenemos el texto crudo (ingredientes sin procesar). \n",
    "La limpieza (Receta 5) es como pelar y cortar los vegetales (eliminar puntuación, stop words \n",
    "y encontrar la raíz de la palabra) para que sean útiles. Luego, Recetas 1 y 2 miden la cantidad \n",
    "general y el tamaño (¿Cuántos ingredientes hay? ¿Cuántas porciones?). Finalmente, BoW y TF-IDF \n",
    "(Recetas 3 y 4) son como catalogar y ponderar la importancia de cada ingrediente: BoW cuenta \n",
    "cuántas veces se usa el ajo (frecuencia simple), mientras que TF-IDF determina qué tan esencial \n",
    "es el azafrán (un ingrediente raro y específico) en esta receta particular en comparación con \n",
    "todas las demás recetas en el libro (la colección de documentos).\n",
    "\n",
    "### Verificación de Archivos Descargados\n",
    "\n",
    "Todos los datos han sido descargados en el directorio del proyecto:\n",
    "- NLTK data: ./nltk_data/\n",
    "- Scikit-learn data: ./sklearn_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf6dcc-885c-40f4-9244-03dc2789b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICACIÓN DE ARCHIVOS DEL PROYECTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDirectorio del proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"\\nArchivos NLTK: {NLTK_DATA_DIR}\")\n",
    "if NLTK_DATA_DIR.exists():\n",
    "    nltk_files = list(NLTK_DATA_DIR.rglob(\"*\"))\n",
    "    print(f\"  ✓ {len(nltk_files)} archivos encontrados\")\n",
    "\n",
    "print(f\"\\nArchivos Scikit-learn: {SKLEARN_DATA_DIR}\")\n",
    "if SKLEARN_DATA_DIR.exists():\n",
    "    sklearn_files = list(SKLEARN_DATA_DIR.rglob(\"*\"))\n",
    "    print(f\"  ✓ {len(sklearn_files)} archivos encontrados\")\n",
    "\n",
    "print(\"\\n✓ Todos los archivos están en el directorio del proyecto\")\n",
    "print(\"  No hay descargas en el home del usuario (~/).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
